{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTd1VBeCFQ7y",
        "outputId": "c509cda7-ef0c-4ae7-b2ff-8615ded97770"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /content/drive/MyDrive/codellama-7b.Q2_K.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = codellama_codellama-7b-hf\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:   65 tensors\n",
            "llama_model_loader: - type q3_K:  160 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32016\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 16384\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
            "llm_load_print_meta: general.name     = codellama_codellama-7b-hf\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2694.39 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'codellama_codellama-7b-hf', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n",
            "Using fallback chat format: None\n",
            "\n",
            "llama_print_timings:        load time =   44426.84 ms\n",
            "llama_print_timings:      sample time =    1464.14 ms /  2143 runs   (    0.68 ms per token,  1463.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =   44426.53 ms /    98 tokens (  453.33 ms per token,     2.21 tokens per second)\n",
            "llama_print_timings:        eval time = 1501752.78 ms /  2142 runs   (  701.10 ms per token,     1.43 tokens per second)\n",
            "llama_print_timings:       total time = 1560520.70 ms /  2240 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\\end{code}\n",
            "\n",
            "Comment: I have added the code that I am trying to implement\n",
            "\n",
            "Comment: what is the result you are getting and what are you expecting.\n",
            "\n",
            "Comment: The results I am getting are not as expected. I am getting 2,0 and I expect to get 1,2. I have a feeling it has something do with the way I have initialized nums array\n",
            "\n",
            "Answer: Here is an easy to follow explanation of what's going on here.\n",
            "\n",
            "\\begin{code}\n",
            "public class Solution {\n",
            "    public int[] twoSum(int[] nums, int target) {\n",
            "        for (int i = 0; i < nums.length; i++) { // for each value in nums\n",
            "            for (int j = i + 1; j < nums.length; j++) { // skip if already used or found it\n",
            "                if (nums[j] == target - nums[i]) { // if the remaining sum is equal to the target\n",
            "                    return new int[] { i, j }; // return the values you're looking for\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "        return null;\n",
            "    }\n",
            "}\n",
            "\\end{code}\n",
            "\n",
            "Let's say we have an input of `nums` and a `target`. The first loop looks at each value in nums. Then it tries to find the next value that would make `nums[i] + nums[j] == target`, with j starting after i, and incrementing by 1 for every additional step. If it finds a pair of numbers like this, then it returns those values.\n",
            "\n",
            "You can also do this in one loop, but you have to be careful about checking if you've already used a value or found the result. I tried doing that myself, and my version was way less performant, so I figured there wasn't any reason not to use two loops.\n",
            "\n",
            "Comment: Thanks for taking the time to explain it. Now I can understand the logic behind the algorithm. Also how would you implement this using a map?\n",
            "\n",
            "Comment: I can't think of a good way right now. The map is really just a hack to make sure that we don't use the same value twice, and with two loops, we're guaranteed not to do that.\n",
            "\n",
            "Answer: This code assumes `nums` is sorted in ascending order (which it usually is) and does the following:\n",
            "\n",
            "\\begin{code}\n",
            "int[] result = new int[2]; // an array of size 2 used to return the results\n",
            "int left = 0;           // left index for searching nums\n",
            "int right = nums.length-1; // right index for searching nums\n",
            "\n",
            "// loop through all pairs of values in nums\n",
            "while (left < right) {\n",
            "    if (nums[right] == target - nums[left]) { // if we find a pair...\n",
            "        result[0] = left; // save the value at index 0 to result\n",
            "        result[1] = right; // save the value at index 1 to result\n",
            "        break; // exit the loop early because we found what we were looking for\n",
            "    } else if (nums[right] < target - nums[left]) { // if the next value in nums is less than our target...\n",
            "        right--; // ... increment left and continue searching from there\n",
            "    } else {                  // if the next value in nums is greater than our target...\n",
            "        left++; // ... decrement right and continue searching from there\n",
            "    }\n",
            "}\n",
            "return result;\n",
            "\\end{code}\n",
            "\n",
            "Comment: Thanks for the explanation! Now I understand what it does. It's very interesting to see how it works. Do you think that using map would be more efficient?\n",
            "\n",
            "Comment: @RahulSharma No, because the search through `nums` is faster than a single lookup in a Map and the code is easier to follow. If we don't find anything in this loop then we exit early - nothing else needs to be done, so there are no side effects or complications from using a map.\n",
            "\n",
            "Comment: @RahulSharma I know it is not efficient but if you want to understand what it does and why, a comment like yours is totally valid.\n",
            "\n",
            "Answer: Here is the explanation of this solution\n",
            "\n",
            "\\begin{code}\n",
            " public int[] twoSum(int[] nums, int target) {\n",
            "        int[] result = new int[2]; // an array of size 2 used to return the results\n",
            "        int left = 0;           // left index for searching nums\n",
            "        int right = nums.length-1; // right index for searching nums\n",
            "\n",
            "        // loop through all pairs of values in nums\n",
            "        while (left < right) {\n",
            "            if (nums[right] == target - nums[left]) { // if we find a pair...\n",
            "                result[0] = left; // save the value at index 0 to result\n",
            "                result[1] = right; // save the value at index 1 to result\n",
            "                break; // exit the loop early because we found what we were looking for\n",
            "            } else if (nums[right] < target - nums[left]) { // if the next value in nums is less than our target...\n",
            "                right--; // ... increment left and continue searching from there\n",
            "            } else {                  // if the next value in nums is greater than our target...\n",
            "                left++; // ... decrement right and continue searching from there\n",
            "            }\n",
            "        }\n",
            "        return result;\n",
            "    }\n",
            "\\end{code}\n",
            "\n",
            "Now let's understand what happens in this solution line by line.\n",
            "\\begin{itemize}\n",
            "\\item `result = new int[2]` : creates a 2 length array that is used to hold the index of the two elements whose sum equals target.\n",
            "\\item \\begin{code}\n",
            "for(int i=0;i<nums.length;i++)\n",
            "\\end{code} : this loop iterates over all the numbers in nums array.\n",
            "\\item `if (nums[right] == target - nums[left])`: checks if there is a pair that adds up to target value.\n",
            "\\end{itemize}\n",
            "\n",
            "You might be wondering why do we check for \\begin{code}\n",
            "i<right && j < left\n",
            "\\end{code} in the `for` loop?\n",
            "\n",
            "This is done to make sure we don't use a number twice. Suppose the array is {2, 7 , 11, 15}, and target is 20. If we just check for \\begin{code}\n",
            "i<nums.length && j < nums.length\n",
            "\\end{code}, this would result in `right=3` being used twice once for i=2 & once for i=3. The solution will work but it would be slower than the expected time complexity of O(n) because we are doing a lot of extra checks (like if the target is 20 and there are 4 numbers, then in total we make around 16 comparisons to check if the sum equals target or not).\n",
            "\n",
            "\\section{Why did I use two loops? Why not just one?}\n",
            "\n",
            "The solution requires two passes. The first pass (i) tells you if there is a pair that adds up to target value and second pass (j) tells you what are those numbers in `nums` array.\n",
            "\n",
            "This can be easily modified to work with just one loop, but this would require extra checks for same values being used twice & not finding the right answer at all. This solution works in O(n) time.\n",
            "\n",
            "Comment: Thanks a lot for taking the time to explain it! I have a question though. What does it mean when you say \"left < right\" and what is the logic behind using two while loops instead of one?\n",
            "\n",
            "Comment: @RahulSharma I updated my answer. Check now :)\n",
            "\n",
            "Comment: Thanks. This was really helpful.\n",
            "\n",
            "Answer: You are iterating over all the pairs from the first element to the last element in `nums` array.\n",
            "\n",
            "For each pair `(i, j)`, you are checking if the difference between `target - nums[i]` and `nums[j]` is equal to `0`. This way you get a pair of numbers that sum up to `target`.\n",
            "\n",
            "Then you return those two numbers in an array.\n",
            "\n",
            "I don't see why would using `Map` be more efficient. The algorithm doesn't even rely on the order of elements in `nums` array, so `Map` is just another structure for storing the values that gets in the way of having a constant time complexity.\n",
            "\n",
            "Comment: Thanks for taking the time to explain it! Now I understand what it does. It's very interesting to see how it works. Do you think that using map would be more efficient?\n",
            "\n",
            "Answer: There are two loops. The outer loop goes over each element of `nums` and the inner loop tries all the numbers in `nums` after the current one. This is done because a number can only show up once in the result, so it mustn't be used twice. For example, if you have {2, 7, 11} as an input and target of 13, then this will find (0, 2) since 2 + 7 = 13.\n",
            "\n",
            "Inside that loop there is a conditional check to see if the number being considered adds up to `target` from `nums[i]`. If it does, it returns the index of `i`. Since we know there can only be one result, and they are in different indices, we're guaranteed to find at least one pair.\n",
            "\n",
            "Comment: Thanks for taking the time to explain\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(model_path=\"/content/drive/MyDrive/codellama-7b.Q2_K.gguf\", n_ctx=4096, n_gpu_layers=35)\n",
        "\n",
        "prompt = prompt = \"Generate code that optimizes the code: class Solution { public int[] twoSum(int[] nums, int target) { for (int i = 0; i < nums.length; i++) { for (int j = i + 1; j < nums.length; j++) { if (nums[j] == target - nums[i]) { return new int[] { i, j }; } } } return null; }}\"\n",
        "\n",
        "output = llm(\n",
        "  prompt,\n",
        "  max_tokens=0,\n",
        "  echo=False\n",
        ")\n",
        "\n",
        "print(output[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO71Eq_ACHK8",
        "outputId": "f339dbc8-5d81-4487-b01d-0f1428f6755f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.62.tar.gz (37.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.5/37.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.62-cp310-cp310-manylinux_2_35_x86_64.whl size=3127774 sha256=66544ee1edd7fdf14c1bf74d41be3c0313463f09656722ddd858142ae331efe3\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/81/de/d4cc8f152d89865379dbf28ca672358c667192ee55deaca7cb\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.62\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1lBoRJUFqcJ"
      },
      "source": [
        "# New section"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}